{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f52bed4",
   "metadata": {},
   "source": [
    "# Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8f781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:08.996197Z",
     "iopub.status.busy": "2024-06-09T02:29:08.996009Z",
     "iopub.status.idle": "2024-06-09T02:29:09.932224Z",
     "shell.execute_reply": "2024-06-09T02:29:09.931708Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e587",
   "metadata": {},
   "source": [
    "## Tensors with all elements being zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9527a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:09.936166Z",
     "iopub.status.busy": "2024-06-09T02:29:09.935956Z",
     "iopub.status.idle": "2024-06-09T02:29:09.941217Z",
     "shell.execute_reply": "2024-06-09T02:29:09.940685Z"
    }
   },
   "outputs": [],
   "source": [
    "# A 1-d all-zero tensor (vector) with 10 elements\n",
    "a = torch.zeros(10)\n",
    "print(f\"{type(a)=}\")\n",
    "print(f\"{a=}\")\n",
    "print(f\"{a.shape=}\")\n",
    "print(f\"{a.ndim=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d4ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:09.945868Z",
     "iopub.status.busy": "2024-06-09T02:29:09.945725Z",
     "iopub.status.idle": "2024-06-09T02:29:09.950170Z",
     "shell.execute_reply": "2024-06-09T02:29:09.949731Z"
    }
   },
   "outputs": [],
   "source": [
    "# A 2-d all-one tensor (matrix) with shape 3,4\n",
    "b = torch.ones((3, 4))\n",
    "print(f\"{type(b)=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"{b.shape=}\")\n",
    "print(f\"{b.ndim=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cfad3",
   "metadata": {},
   "source": [
    "## Representing a color image with 3-D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ad1e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:09.953426Z",
     "iopub.status.busy": "2024-06-09T02:29:09.953282Z",
     "iopub.status.idle": "2024-06-09T02:29:09.988234Z",
     "shell.execute_reply": "2024-06-09T02:29:09.987434Z"
    }
   },
   "outputs": [],
   "source": [
    "# We first use PIL.Image to read an image and visualize it\n",
    "img_pil = Image.open(\"cat.jpg\")\n",
    "img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29560938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:09.992558Z",
     "iopub.status.busy": "2024-06-09T02:29:09.992395Z",
     "iopub.status.idle": "2024-06-09T02:29:09.997390Z",
     "shell.execute_reply": "2024-06-09T02:29:09.996905Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can convert this PIL image to a tensor with shape (H, W, C),\n",
    "# where each element (pixel) is an uint8 (0-255).\n",
    "# Unfortunately PyTorch cannot load PIL images directly,\n",
    "# hence we need to convert them to a numpy array first.\n",
    "img_array = torch.as_tensor(np.array(img_pil))\n",
    "print(f\"{img_array=}\")\n",
    "print(f\"{img_array.shape=}\")\n",
    "print(f\"{img_array.dtype=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb13b4",
   "metadata": {},
   "source": [
    "## Manipulating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e7b17",
   "metadata": {},
   "source": [
    "In PyTorch, element-wise operation is written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b93a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:09.999304Z",
     "iopub.status.busy": "2024-06-09T02:29:09.999165Z",
     "iopub.status.idle": "2024-06-09T02:29:10.005473Z",
     "shell.execute_reply": "2024-06-09T02:29:10.004876Z"
    }
   },
   "outputs": [],
   "source": [
    "# elementwise operation\n",
    "a = torch.tensor([0, 1, 2])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(f\"{a=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"elementwise addition: {a + b=}\")  # equivalent to `torch.add(a, b)`\n",
    "print(f\"elementwise substraction: {a - b=}\")  # equivalent to `torch.sub(a, b)`\n",
    "print(f\"elementwise multiplication: {a * b=}\")  # equivalent to `torch.mul(a, b)`\n",
    "print(f\"elementwise division: {a / b=}\")  # equivalent to `torch.div(a, b)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde946f",
   "metadata": {},
   "source": [
    "For all element-wise operations, make sure both tensors have the same shape.\n",
    "Otherwise the operation will fail and raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c309ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.007382Z",
     "iopub.status.busy": "2024-06-09T02:29:10.007190Z",
     "iopub.status.idle": "2024-06-09T02:29:10.017184Z",
     "shell.execute_reply": "2024-06-09T02:29:10.016820Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We generate two tensor with shape of 5 and 6 filled with random numbers\n",
    "# from a uniform distribution on the interval $[0, 1)$.\n",
    "# In this case, the element-wise addition operation will fail and raise an error.\n",
    "a = torch.rand(5)\n",
    "b = torch.rand(6)\n",
    "try:\n",
    "    print(f\"{a + b=}\")\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853eb688",
   "metadata": {},
   "source": [
    "The cool thing about tensor is we can run a large number of operations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162356e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.018999Z",
     "iopub.status.busy": "2024-06-09T02:29:10.018860Z",
     "iopub.status.idle": "2024-06-09T02:29:10.037375Z",
     "shell.execute_reply": "2024-06-09T02:29:10.036708Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1_000_000)\n",
    "y = torch.rand(1_000_000)\n",
    "print(f\"{x + y=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254c3cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.043159Z",
     "iopub.status.busy": "2024-06-09T02:29:10.042915Z",
     "iopub.status.idle": "2024-06-09T02:29:10.045921Z",
     "shell.execute_reply": "2024-06-09T02:29:10.045412Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b28e6",
   "metadata": {},
   "source": [
    "## View vs Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c28c5",
   "metadata": {},
   "source": [
    "Both operations manipuate the tensor and return a tensor with the same data with a specific shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d11ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.048795Z",
     "iopub.status.busy": "2024-06-09T02:29:10.048535Z",
     "iopub.status.idle": "2024-06-09T02:29:10.053465Z",
     "shell.execute_reply": "2024-06-09T02:29:10.052964Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(6)\n",
    "print(f\"{a=}\")\n",
    "print(f\"{a.view(2, 3)=}\")\n",
    "print(f\"{a.reshape(2, 3)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c30b9",
   "metadata": {},
   "source": [
    "These two operations can yield similar outcomes but can work differently.\\\n",
    "`torch.view` merely creates a view of the original tensor and shares the underling data\n",
    "with the original tensor.\n",
    "To ensure the data sharing, `torch.view` can only operate on *contiguous* tensors,\n",
    "meaning that the tensors to be viewed must are stored in contiguous memory.\\\n",
    "In contrast, `torch.reshape` does not have this constraint.\n",
    "Whenever possible, the returned tensor will be a view of input.\n",
    "Otherwise, it will be a copy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed99949",
   "metadata": {},
   "source": [
    "A simple non-contigous case may arise because of transpose operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762f680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.056226Z",
     "iopub.status.busy": "2024-06-09T02:29:10.056036Z",
     "iopub.status.idle": "2024-06-09T02:29:10.069754Z",
     "shell.execute_reply": "2024-06-09T02:29:10.069238Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = a.t()\n",
    "print(f\"{a=}\")\n",
    "try:\n",
    "    print(f\"{b.view(6)=}\")\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "print(f\"{b.contiguous().view(6)=}\")\n",
    "print(f\"{b.reshape(6)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6078099",
   "metadata": {},
   "source": [
    "## Permute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa50a46",
   "metadata": {},
   "source": [
    "`torch.permute` returns a view of the original tensor input with its dimensions permuted to have a desired ordering.\n",
    "It can be thought of a generalized tranpose operation in N-D tensor.\n",
    "\n",
    "Please beware that `torch.permute` is *NOT* the same as `torch.view`\n",
    "even though they return tensors with same shapes sometimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216ee09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.072434Z",
     "iopub.status.busy": "2024-06-09T02:29:10.072243Z",
     "iopub.status.idle": "2024-06-09T02:29:10.076663Z",
     "shell.execute_reply": "2024-06-09T02:29:10.076135Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(f\"{a.view(3, 2)=}\")\n",
    "print(f\"{a.permute(1, 0)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c977edd",
   "metadata": {},
   "source": [
    "## Squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cec75",
   "metadata": {},
   "source": [
    "`torch.squeeze` returns a tensor with all specified dimensions of input of size 1 removed.\n",
    "Please note that you should NEVER call `torch.squeeze` without an argument specifying the dimensions to be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb81d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.079887Z",
     "iopub.status.busy": "2024-06-09T02:29:10.079690Z",
     "iopub.status.idle": "2024-06-09T02:29:10.083601Z",
     "shell.execute_reply": "2024-06-09T02:29:10.083085Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(3, 1, 1, 2)\n",
    "print(f\"{a.squeeze(1).shape=}\")\n",
    "print(f\"{a.squeeze((1, 2)).shape=}\")\n",
    "print(f\"{a.squeeze().shape=}\")  # Illustration purpose only! You should NEVER use this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855ea58",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cae502",
   "metadata": {},
   "source": [
    "The contents of a tensor can be accessed and modified using Python’s indexing and slicing notation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d15b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.087371Z",
     "iopub.status.busy": "2024-06-09T02:29:10.086893Z",
     "iopub.status.idle": "2024-06-09T02:29:10.092579Z",
     "shell.execute_reply": "2024-06-09T02:29:10.092131Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(f\"{a=}\")\n",
    "print(f\"{a[1, 0]=}\")\n",
    "print(f\"{a[1, :]=}\")\n",
    "print(f\"{a[:, 2]=}\")\n",
    "print(a[:, [0, 2]])  # equivalent to torch.index_select(a, 1, torch.tensor([0, 2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de22f4",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097def80",
   "metadata": {},
   "source": [
    "## Broadcasting - Outer-product-like addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be4e73",
   "metadata": {},
   "source": [
    "Let's start from an example of outer product.\n",
    "It can be implemented with a for-loop or an in-built function `torch.outer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de0e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.096656Z",
     "iopub.status.busy": "2024-06-09T02:29:10.096303Z",
     "iopub.status.idle": "2024-06-09T02:29:10.101934Z",
     "shell.execute_reply": "2024-06-09T02:29:10.101423Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6)\n",
    "b = torch.arange(5)\n",
    "c = torch.zeros((6, 5), dtype=a.dtype)\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        c[i, j] = a[i] * b[j]\n",
    "print(f\"{c=}\")\n",
    "print(f\"{torch.outer(a, b)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e17a74",
   "metadata": {},
   "source": [
    "As you can see above, the outer product multiplies each pair of numbers from the two input tensors.\n",
    "\n",
    "Let's say we want to add (instead of multiply) each pair of elements,\n",
    "there is no such operation in PyTorch that can do this directly.\n",
    "A slightly painful way to implement this is to use a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd83e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.104403Z",
     "iopub.status.busy": "2024-06-09T02:29:10.104076Z",
     "iopub.status.idle": "2024-06-09T02:29:10.110977Z",
     "shell.execute_reply": "2024-06-09T02:29:10.110295Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6)\n",
    "b = torch.arange(5)\n",
    "c = torch.zeros((6, 5), dtype=a.dtype)\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        c[i, j] = a[i] + b[j]\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba66d8",
   "metadata": {},
   "source": [
    "This, however, is not very efficient and is a lot of code.\n",
    "\n",
    "PyTorch allows you to do this more efficiently using *broadcasting*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1175c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.113325Z",
     "iopub.status.busy": "2024-06-09T02:29:10.113066Z",
     "iopub.status.idle": "2024-06-09T02:29:10.116994Z",
     "shell.execute_reply": "2024-06-09T02:29:10.116480Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6)\n",
    "b = torch.arange(5)\n",
    "print(a[:, None] + b[None, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81556dff",
   "metadata": {},
   "source": [
    "Let's unpack our example of outer-product-like addition.\n",
    "First, we can add a new dimension of shape 1 to a tensor by passing a `None` index like below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d24bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.119460Z",
     "iopub.status.busy": "2024-06-09T02:29:10.119112Z",
     "iopub.status.idle": "2024-06-09T02:29:10.123258Z",
     "shell.execute_reply": "2024-06-09T02:29:10.122751Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "print(f\"{a.shape=}\")\n",
    "print(f\"{a[None].shape=}\")\n",
    "print(f\"{a[:, None].shape=}\")\n",
    "print(f\"{a[:, None, None].shape=}\")\n",
    "print(f\"{a[None, :, None].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e3950",
   "metadata": {},
   "source": [
    "After that, a tensor whose shape dimension is 1 can be expanded (or *broadcast*ed).\n",
    "In this example of `a[:, None] + b[None, :]`, `a[:, None]` and `b[None, :]` has shape (6,1)\n",
    "and shape (1, 5) respectively, so `a` and `b` are *broadcastable* and the resulting tensor will be expanded to (6, 5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41cc4f",
   "metadata": {},
   "source": [
    "## Finding maximum distance between points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f23949",
   "metadata": {},
   "source": [
    "Let's look at another example.\n",
    "Assume that we randomly generate a set of 100 2-D points from a 2-D space following\n",
    "standard normal distribution (`x=torch.randn(100, 2)`).\n",
    "We want to find the maximum distance between points.\n",
    "\n",
    "A naive way is to compute the pairwise distance using a nested loop and compute the maximum thereon.\n",
    "With broadcasting, we can do it in one line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57928f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.125619Z",
     "iopub.status.busy": "2024-06-09T02:29:10.125431Z",
     "iopub.status.idle": "2024-06-09T02:29:10.237495Z",
     "shell.execute_reply": "2024-06-09T02:29:10.237039Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(100, 2)\n",
    "d = torch.zeros(100, 100)\n",
    "\n",
    "# For-loop\n",
    "for i in range(100):\n",
    "    for j in range(i, 100):\n",
    "        d[i, j] = ((x[i] - x[j]) ** 2).sum().sqrt()\n",
    "print(f\"{torch.max(d)=}\")\n",
    "\n",
    "# One-line with broadcasting\n",
    "((x[:, None, :] - x[None, :, :]) ** 2).sum(2).sqrt().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ada9f",
   "metadata": {},
   "source": [
    "## Matrix multiplcation in batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be89401",
   "metadata": {},
   "source": [
    "Let's see another example.\n",
    "Assume we want have a 3-d tensor `a` and a 2-d matrix `b`.\n",
    "For each slice of `a` (`a[i, ...]`), we want to multiply it with matrix ` b`.\n",
    "A naive way is to do the for loop but it's super slow when the number of slices becomes large.\n",
    "With broadcasting, the operation is written in one line and executes faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07656f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:10.239684Z",
     "iopub.status.busy": "2024-06-09T02:29:10.239505Z",
     "iopub.status.idle": "2024-06-09T02:29:16.253967Z",
     "shell.execute_reply": "2024-06-09T02:29:16.253465Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn(100, 50, 200)\n",
    "b = torch.randn(200, 100)\n",
    "\n",
    "c = torch.empty(100, 50, 100)\n",
    "\n",
    "%timeit for i in range(100): c[i, ...] = a[i, ...] @ b\n",
    "%timeit a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d7f3b",
   "metadata": {},
   "source": [
    "The gain is more significant if we move the tensor to the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d95b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:16.256146Z",
     "iopub.status.busy": "2024-06-09T02:29:16.255970Z",
     "iopub.status.idle": "2024-06-09T02:29:25.985958Z",
     "shell.execute_reply": "2024-06-09T02:29:25.985318Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # note this will fail if you don't have a GPU\n",
    "    a = a.cuda()\n",
    "    b = b.cuda()\n",
    "    c = c.cuda()\n",
    "    %timeit for i in range(100): c[i, ...] = a[i, ...] @ b\n",
    "    %timeit a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab69a13",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74b2cb",
   "metadata": {},
   "source": [
    "## Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1fd30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:25.989178Z",
     "iopub.status.busy": "2024-06-09T02:29:25.989002Z",
     "iopub.status.idle": "2024-06-09T02:29:25.993121Z",
     "shell.execute_reply": "2024-06-09T02:29:25.992668Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(2, 4)\n",
    "b = torch.rand(4, 3)\n",
    "print(f\"{a @ b=}\")  # equivalent to `torch.matmul(a, b)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0228ec",
   "metadata": {},
   "source": [
    "Beware of the dimensions of matrices. The number of columns of tensor a must be equal to\n",
    "the number of rows of tensor b. The example below does not work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298dc0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:25.995195Z",
     "iopub.status.busy": "2024-06-09T02:29:25.994873Z",
     "iopub.status.idle": "2024-06-09T02:29:26.004785Z",
     "shell.execute_reply": "2024-06-09T02:29:26.004151Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(5, 5)\n",
    "b = torch.rand(3, 5)\n",
    "try:\n",
    "    print(f\"{a @ b=}\")\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9adfe61",
   "metadata": {},
   "source": [
    "## Vector multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726715b",
   "metadata": {},
   "source": [
    "Let's first look at vector and matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a01793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.006721Z",
     "iopub.status.busy": "2024-06-09T02:29:26.006576Z",
     "iopub.status.idle": "2024-06-09T02:29:26.010620Z",
     "shell.execute_reply": "2024-06-09T02:29:26.010213Z"
    }
   },
   "outputs": [],
   "source": [
    "M = torch.rand(3, 3)\n",
    "v = torch.rand(3, 1)\n",
    "\n",
    "print(f\"matrix: {M=}\")\n",
    "print(f\"column vector: {v=}\")\n",
    "\n",
    "print(f\"column vector can be multiplied on the right {M @ v=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92cc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.014289Z",
     "iopub.status.busy": "2024-06-09T02:29:26.014117Z",
     "iopub.status.idle": "2024-06-09T02:29:26.017582Z",
     "shell.execute_reply": "2024-06-09T02:29:26.017125Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "w = torch.rand(1, 3)\n",
    "\n",
    "print(f\"row vector: {w=}\")\n",
    "\n",
    "print(f\"row vector can be multiplied on the left {w @ M=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a666e",
   "metadata": {},
   "source": [
    "Let's see vector-vector multiplication next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4366bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.021753Z",
     "iopub.status.busy": "2024-06-09T02:29:26.021597Z",
     "iopub.status.idle": "2024-06-09T02:29:26.025393Z",
     "shell.execute_reply": "2024-06-09T02:29:26.024953Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Define two column vectors\")\n",
    "\n",
    "a = torch.rand(3, 1)\n",
    "b = torch.rand(3, 1)\n",
    "\n",
    "print(f\"{a=}\")\n",
    "print(f\"{b=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0684df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.028812Z",
     "iopub.status.busy": "2024-06-09T02:29:26.028668Z",
     "iopub.status.idle": "2024-06-09T02:29:26.031636Z",
     "shell.execute_reply": "2024-06-09T02:29:26.031198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inner product\n",
    "print(\"Inner product\")\n",
    "print(f\"{a.T @ b=}\")  # equivalent to `torch.dot(a.T, b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5f206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.033483Z",
     "iopub.status.busy": "2024-06-09T02:29:26.033340Z",
     "iopub.status.idle": "2024-06-09T02:29:26.036383Z",
     "shell.execute_reply": "2024-06-09T02:29:26.035938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Outer product\n",
    "print(\"Outer product\")\n",
    "print(f\"{a @ b.T=}\")  # equivalent to `torch.dot(a, b.T)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94301c9",
   "metadata": {},
   "source": [
    "## Euclidean norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89c750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.038301Z",
     "iopub.status.busy": "2024-06-09T02:29:26.038129Z",
     "iopub.status.idle": "2024-06-09T02:29:26.042268Z",
     "shell.execute_reply": "2024-06-09T02:29:26.041828Z"
    }
   },
   "outputs": [],
   "source": [
    "v = torch.rand(3, 1)\n",
    "\n",
    "\n",
    "print(f\"{v=}\")\n",
    "\n",
    "print(\"we can compute the norm of a vector with definition\")\n",
    "print(f\"{(v ** 2).sum().sqrt()=}\")\n",
    "\n",
    "print(\"or with torch.norm\")\n",
    "\n",
    "print(f\"{torch.norm(v)=}\")\n",
    "print(\"warning: using torch.norm is slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12090a31",
   "metadata": {},
   "source": [
    "## Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866e2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:29:26.044171Z",
     "iopub.status.busy": "2024-06-09T02:29:26.043999Z",
     "iopub.status.idle": "2024-06-09T02:29:26.048174Z",
     "shell.execute_reply": "2024-06-09T02:29:26.047702Z"
    }
   },
   "outputs": [],
   "source": [
    "W = torch.rand(3, 3)\n",
    "\n",
    "print(f\"{W=}\")\n",
    "\n",
    "print(\"we can compute the norm of a matrix with definition\")\n",
    "\n",
    "print(f\"{torch.sqrt(torch.sum(W ** 2))=}\")\n",
    "\n",
    "print(\"or with torch.norm\")\n",
    "\n",
    "print(f\"{torch.norm(W)=}\")\n",
    "print(\"warning: using torch.norm is slower\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "title"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "title": "Tensors in PyTorch"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
